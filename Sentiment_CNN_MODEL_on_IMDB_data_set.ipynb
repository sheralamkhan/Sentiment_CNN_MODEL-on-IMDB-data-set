{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TcBjLXSlIig"
      },
      "source": [
        "# CNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a2CPKKdqtr4o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vEpWFXK3t7Xl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f40e5d29-bd7e-4ca8-b108-4350a601397c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"df = pd.read_csv('/content/IMDB_Dataset.csv')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''df = pd.read_csv('/content/IMDB_Dataset.csv')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_S1cL-ulNp4",
        "outputId": "342a9d5a-0e90-4f61-a84b-fd826b9fbc11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7FWTgSaPlIil"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "with open('/content/drive/MyDrive/reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('/content/drive/MyDrive/labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogsQDzo6lIin",
        "outputId": "b48d4208-6b4c-4f4d-8016-dd1d6fc7fba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which\n",
            "\n",
            "positive\n",
            "negative\n",
            "po\n"
          ]
        }
      ],
      "source": [
        "print(reviews[:500])\n",
        "print()\n",
        "print(labels[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AqhiN-8lIio"
      },
      "source": [
        "---\n",
        "## Data Pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3L6zAM3ZuvjL",
        "outputId": "5a18a7d2-a645-46e9-bf2b-d836d7cdf234"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KkOqL1vvlIio"
      },
      "outputs": [],
      "source": [
        "\n",
        "from string import punctuation\n",
        "\n",
        "# remove punctuation\n",
        "reviews = reviews.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
        "\n",
        "# split by new lines and spaces\n",
        "reviews_split = all_text.split('\\n')\n",
        "\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of all words\n",
        "all_words = all_text.split()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN4LB5cdlIip"
      },
      "source": [
        "### Encoding the Labels\n",
        "\n",
        "Convert \"positive\" or \"negative\" labels to numerical values, 1 (positive) and 0 (negative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BEg5t4C8lIip"
      },
      "outputs": [],
      "source": [
        "# 1=positive, 0=negative label conversion\n",
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3pHpGkolIiq"
      },
      "source": [
        "### Removing Outliers\n",
        "\n",
        "Remove outliers in following steps\n",
        "1. Getting rid of extremely long or short reviews; the outliers\n",
        "2. Padding/truncating the remaining data so that all reviews are of the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWX1Q0zglIiq",
        "outputId": "0509f3d1-a454-4ace-acab-9b9adc6f851d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-length reviews: 1\n",
            "Maximum review length: 2514\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Build a dictionary that maps indices to review lengths\n",
        "counts = Counter(all_words)\n",
        "\n",
        "review_lens = Counter([len(x.split()) for x in reviews_split])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnGuNAbplIir",
        "outputId": "a16126a1-450c-41aa-c3a3-d6c5eeb54ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ]
        }
      ],
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_split))\n",
        "\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_split) if len(review.split()) != 0]\n",
        "\n",
        "reviews_split = [reviews_split[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_split))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbL-x6wBlIir"
      },
      "source": [
        "---\n",
        "## Using a Pre-Trained Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3k6al07flIir"
      },
      "outputs": [],
      "source": [
        "# Load Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Creating the model\n",
        "embed_lookup = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300-SLIM.bin',\n",
        "                                                 binary=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZDb2s2KlIir"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3-OV5PR_lIis"
      },
      "outputs": [],
      "source": [
        "pretrained_words = []\n",
        "for index, word in enumerate(embed_lookup.index_to_key):\n",
        "    pretrained_words.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "siI6F6zGlIis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278fd9ad-ea07-4ea8-90a7-0acb571919be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Vocab: 299567\n",
            "\n",
            "Word in vocab: for\n",
            "\n",
            "Length of embedding: 300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "row_idx = 1\n",
        "\n",
        "# get word/embedding in that row\n",
        "word = pretrained_words[row_idx]\n",
        "embedding = embed_lookup[word]\n",
        "\n",
        "# vocab and embedding info\n",
        "print(\"Size of Vocab: {}\\n\".format(len(pretrained_words)))\n",
        "print('Word in vocab: {}\\n'.format(word))\n",
        "print('Length of embedding: {}\\n'.format(len(embedding)))\n",
        "#print('Associated embedding: \\n', embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K1W4FFOelIis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3621a884-dbdc-465e-fb08-50940c94cdfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in\n",
            "for\n",
            "that\n",
            "is\n",
            "on\n"
          ]
        }
      ],
      "source": [
        "# print a few common words\n",
        "for i in range(5):\n",
        "    print(pretrained_words[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX_lIjkdlIis"
      },
      "source": [
        "### Cosine Similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gxlU90F9lIis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5666eba9-1a49-42cd-8fce-7a784bbea6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar words to fabulous: \n",
            "\n",
            "Word: wonderful, Similarity: 0.761\n",
            "Word: fantastic, Similarity: 0.761\n",
            "Word: marvelous, Similarity: 0.730\n",
            "Word: gorgeous, Similarity: 0.714\n",
            "Word: lovely, Similarity: 0.713\n",
            "Word: terrific, Similarity: 0.694\n",
            "Word: amazing, Similarity: 0.693\n",
            "Word: beautiful, Similarity: 0.670\n",
            "Word: magnificent, Similarity: 0.667\n",
            "Word: splendid, Similarity: 0.645\n"
          ]
        }
      ],
      "source": [
        "find_similar_to = 'fabulous'\n",
        "\n",
        "print('Similar words to '+find_similar_to+': \\n')\n",
        "\n",
        "# Find similar words, using cosine similarity\n",
        "for similar_word in embed_lookup.similar_by_word(find_similar_to):\n",
        "    print(\"Word: {0}, Similarity: {1:.3f}\".format(\n",
        "        similar_word[0], similar_word[1]\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySCarwu7lIit"
      },
      "source": [
        "## Tokenize reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UOomqyAOlIit"
      },
      "outputs": [],
      "source": [
        "def tokenize_all_reviews(embed_lookup, reviews_split):\n",
        "    reviews_words = [review.split() for review in reviews_split]\n",
        "\n",
        "    tokenized_reviews = []\n",
        "    for review in reviews_words:\n",
        "        ints = []\n",
        "        for word in review:\n",
        "            try:\n",
        "                idx = embed_lookup.key_to_index[word]\n",
        "            except:\n",
        "                idx = 0\n",
        "            ints.append(idx)\n",
        "        tokenized_reviews.append(ints)\n",
        "\n",
        "    return tokenized_reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UEA5759zlIit"
      },
      "outputs": [],
      "source": [
        "tokenized_reviews = tokenize_all_reviews(embed_lookup, reviews_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lKqdd3w9lIit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f207610-d459-4bf8-d89b-43d38cc1fad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 137, 3, 0, 11620, 3799, 13, 1215, 10, 9, 194, 54, 12, 73, 61, 685, 41, 183, 243, 129, 12, 1663, 119, 72, 0, 9, 2989, 7334, 242, 159, 0, 453, 2, 0, 137, 1239, 19951, 3, 141, 1980, 0, 1898, 55, 3, 1663, 9, 11124, 0, 3857, 6663, 9, 20401, 295, 28, 45, 148, 157, 102, 27, 15452, 1663, 30714, 9, 65172, 0, 9, 844, 737, 47, 6585, 159, 0, 9, 668, 4365, 1003, 0, 27, 295, 56, 4365, 622, 9, 3832, 0, 43, 0, 897, 3187, 907, 0, 5396, 113, 9, 183, 4365, 1009, 3165, 10, 137, 0, 3288, 296, 10314, 4365, 6638, 213, 0, 8810, 40, 0, 116, 1663, 897, 2059, 0, 0, 137, 4365, 830, 2, 124, 2216, 0, 119, 782, 144, 2, 0, 137, 3, 330, 23046, 78, 0, 16915, 2, 13, 85275, 7451]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_reviews[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgfaBfyelIit"
      },
      "source": [
        "---\n",
        "## Padding sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cajVQFiUlIit"
      },
      "outputs": [],
      "source": [
        "def pad_features(tokenized_reviews, seq_length):\n",
        "    features = np.zeros((len(tokenized_reviews), seq_length), dtype=int)\n",
        "    for i, row in enumerate(tokenized_reviews):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VZaJJsVclIit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d4004a-ccf4-4b44-d517-20e25b510901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [ 16483     26      0     12 106210      0   1698     22     37     24\n",
            "     432      1     72     30    275      0    303      0    162    126]\n",
            " [  1935   1326     12      0   1403     60   3921   2019      3   4809\n",
            "      36      6   3172   7184    129   7951      0   2180   6098 166268]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [    56   4365      8    270    119    756    247    159    381      0\n",
            "       9   2669      0    148  21621     13      8     40      0    124]]\n"
          ]
        }
      ],
      "source": [
        "seq_length = 200\n",
        "\n",
        "features = pad_features(tokenized_reviews, seq_length=seq_length)\n",
        "assert len(features)==len(tokenized_reviews), \"Features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "print(features[:10,:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY38TOFJlIiu"
      },
      "source": [
        "---\n",
        "## Training, Validation, and Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0StQQBIRlIiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52644077-7283-4709-bbf8-4273e68b6a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(20000, 200) \n",
            "Validation set: \t(2500, 200) \n",
            "Test set: \t\t(2500, 200)\n"
          ]
        }
      ],
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPKtndBYlIiu"
      },
      "source": [
        "## DataLoaders and Batching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zWhOiElwlIiu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# shuffling and batching data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DP6qrmflIiu"
      },
      "source": [
        "---\n",
        "# Sentiment Network with PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZmE_U3pllIiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527e8402-7b5c-416b-9125-5119b4a23a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU.\n"
          ]
        }
      ],
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bQ1ylRIFlIiv"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SentimentCNN(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n",
        "                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n",
        "\n",
        "        super(SentimentCNN, self).__init__()\n",
        "\n",
        "        self.num_filters = num_filters\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n",
        "        if freeze_embeddings:\n",
        "            self.embedding.requires_grad = False\n",
        "\n",
        "        self.convs_1d = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2,0))\n",
        "            for k in kernel_sizes])\n",
        "\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "\n",
        "        x = F.relu(conv(x)).squeeze(3)\n",
        "\n",
        "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x_max\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        embeds = embeds.unsqueeze(1)\n",
        "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
        "        x = torch.cat(conv_results, 1)\n",
        "        x = self.dropout(x)\n",
        "        logit = self.fc(x)\n",
        "        return self.sig(logit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWqCOFH-lIiv"
      },
      "source": [
        "## Instantiate the network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QSP6hROzlIiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ed1bd5-eade-4ffe-db62-d2563d98e6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentCNN(\n",
            "  (embedding): Embedding(299567, 300)\n",
            "  (convs_1d): ModuleList(\n",
            "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
            "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
            "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
            "  )\n",
            "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(pretrained_words)\n",
        "output_size = 1\n",
        "embedding_dim = len(embed_lookup[pretrained_words[0]])\n",
        "num_filters = 100\n",
        "kernel_sizes = [3, 4, 5]\n",
        "\n",
        "net3 = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
        "                   num_filters, kernel_sizes)\n",
        "\n",
        "print(net3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46hpm2m2lIiw"
      },
      "source": [
        "---\n",
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KZim6IPklIiz"
      },
      "outputs": [],
      "source": [
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net3.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7pmlur0PlIi0"
      },
      "outputs": [],
      "source": [
        "def train(net, train_loader, epochs, print_every=100):\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    net.train()\n",
        "    for e in range(epochs):\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            counter += 1\n",
        "\n",
        "            if(train_on_gpu):\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            net.zero_grad()\n",
        "\n",
        "            output = net(inputs)\n",
        "\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for inputs, labels in valid_loader:\n",
        "\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                    output = net(inputs)\n",
        "                    val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9nt4sGCQlIi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3e7d72f-48dc-4ba5-885a-9899a9f7e4d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/2... Step: 100... Loss: 0.404961... Val Loss: 0.456944\n",
            "Epoch: 1/2... Step: 200... Loss: 0.478754... Val Loss: 0.371079\n",
            "Epoch: 1/2... Step: 300... Loss: 0.331406... Val Loss: 0.342721\n",
            "Epoch: 1/2... Step: 400... Loss: 0.302353... Val Loss: 0.330672\n",
            "Epoch: 2/2... Step: 500... Loss: 0.204000... Val Loss: 0.344079\n",
            "Epoch: 2/2... Step: 600... Loss: 0.239894... Val Loss: 0.371066\n",
            "Epoch: 2/2... Step: 700... Loss: 0.246180... Val Loss: 0.364074\n",
            "Epoch: 2/2... Step: 800... Loss: 0.155306... Val Loss: 0.360754\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "epochs = 2\n",
        "print_every = 100\n",
        "\n",
        "train(net3, train_loader, epochs, print_every=print_every)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc8flyxblIi0"
      },
      "source": [
        "---\n",
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "u_jr39QQlIi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74bb318c-14d7-4753-ed2f-52fd0b106dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.404\n",
            "Test accuracy: 0.831\n"
          ]
        }
      ],
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "\n",
        "net3.eval()\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    output = net3(inputs)\n",
        "\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W4pnI69lIi0"
      },
      "source": [
        "### Inference on a test review\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "breL-L9OlIi1"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(embed_lookup, test_review):\n",
        "    test_review = test_review.lower()\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "\n",
        "    test_words = test_text.split()\n",
        "\n",
        "    tokenized_review = []\n",
        "    for word in test_words:\n",
        "        try:\n",
        "            idx = embed_lookup.vocab[word].index\n",
        "        except:\n",
        "            idx = 0\n",
        "        tokenized_review.append(idx)\n",
        "\n",
        "    return tokenized_review\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4ofuUpVSlIi1"
      },
      "outputs": [],
      "source": [
        "def predict(embed_lookup, net3, test_review, sequence_length=200):\n",
        "\n",
        "\n",
        "    net3.eval()\n",
        "    print(test_review)\n",
        "    test_ints = tokenize_review(embed_lookup, test_review)\n",
        "\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features([test_ints], seq_length)\n",
        "\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "\n",
        "    batch_size = feature_tensor.size(0)\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "\n",
        "    output = net3(feature_tensor)\n",
        "\n",
        "    pred = torch.round(output.squeeze())\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "\n",
        "    if(pred.item()==1):\n",
        "        print(\"Positive review detected!\")\n",
        "    else:\n",
        "        print(\"Negative review detected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW45V1BKlIi1"
      },
      "source": [
        "### Test on pos/neg reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7WwfQGHdlIi1"
      },
      "outputs": [],
      "source": [
        "seq_length=200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xtO5Nm8plIi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0d54bb-9647-4194-a29e-72e394a906b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie had the best acting and the dialogue was so great. I loved it.\n",
            "Prediction value, pre-rounding: 0.517462\n",
            "Positive review detected!\n"
          ]
        }
      ],
      "source": [
        "test_review_pos = 'This movie had the best acting and the dialogue was so great. I loved it.'\n",
        "\n",
        "predict(embed_lookup, net3, test_review_pos, seq_length)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}